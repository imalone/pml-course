---
title: "Predicting quality of exercise action"
author: "Ian"
date: "17 January 2016"
output: html_document
---

# Summary
A weight lifting exercise dataset, by [Velloso et al.][Velloso] was studied. This contained  on-body sensing data for 6 participants, performing a lifting task categorised by how well/the type of mistakes made (performance classe). Our goal was to predict the performance class of 20 randomly selected and blinded observations.

The un-blinded data were split into training, validation and testing subsets. After exploratory analysis on the training set it was used to build a range of predictive models in an automated fashion using the Caret package. The performance of these was compared on the validation set and the best performing one (per-user Stochastic Gradient Boosting) selected for use in predicting the class of the blinded test observations. Its expected accuracy for this was assessed using 10-fold cross validation on the building set (0.9940) and prediction accuracy on the remaining hold-out test data (0.9954), 

[Velloso]: http://groupware.les.inf.puc-rio.br/har#ixzz3xpfM3Y37 "(Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.)"

# Data description

In the original dataset, the participants carried out repetitions of a single dumbbell arm curl task under supervision. They were either carried out correctly (class A) or in a range of intentionally incorrect manners (classes B-E) under supervision. Sensors were mounted on the glove, armband, belt and dumbbell, each reporting acceleration, magnetometer and gyroscope information on three axes. The original researchers also divided the observations (at 45Hz) into time windows and calculated summary statistics for each time window. The data were retrieved from the Coursera Practical Machine Learning exercise and loaded into R.

```{r libraries, warning=FALSE, results='hide', echo=FALSE}
# Can't include in cached chunks.
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)

library(caret, quietly=TRUE)
library(doMC, quietly=TRUE)

library(knitr, quietly=TRUE)
library(gridExtra,quietly=TRUE)
library(pander)
opts_chunk$set("echo"=FALSE)
```


```{r loaddata, cache=TRUE}
pmlData<-read.csv("pml-training.csv",as.is=TRUE)
pmlDataTest<-read.csv("pml-testing.csv",as.is=TRUE)
pmlDataFilt<-filter(pmlData,new_window=="no")
delcol <- apply(pmlDataFilt,2,function(x){length(unique(x))==1})
delcolind <- which(delcol)
delcolnames <- names(pmlData)[delcol]

pmlData <- pmlData %>%
  select(-delcolind) %>%
  mutate(classe=as.factor(classe),user_name=as.factor(user_name))

pmlDataTest <- pmlDataTest %>%
  mutate(user_name=as.factor(user_name))

pmlDataSel <- pmlData %>%
  select(-c(X, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp,
            num_window))
```

The original data has 160 variables from different sensors, and time-window summary data for the last observation in each time window. Since the prediction task we are addressing does not include full time series data, but only single observations, we discarded these summary variables by removing all fields that have only one unique value when the "new window" observations are omitted. Additionally the fields relating to observation time were removed (row number X, raw\_timestamp\_part\_1, raw\_timestamp\_part\_2, cvtd\_timestamp and num\_window). These variables had already been removed from the set to be predicted.

This left 54 variables including the "classe" variable, indicating how the task was carried out, which we wish to be able to predict. The 106 fields removed are: the 96 new features described in Velloso et al. (for four devices, eight parameters for each Euler angle (3)), the five fields mentioned already, new\_window (whether the observation includes window summary) and four additional derived features corresponding to average acceleration for each measurement device, not mentioned in Velloso et al.

# Data partitioning and exploratory analysis

```{r trainingset}
set.seed(500)
inBuild <- createDataPartition(pmlDataSel$classe,p=0.8,list=FALSE)
building <- pmlDataSel[inBuild,]
testing <- pmlDataSel[-inBuild,]
inTrain <- createDataPartition(building$classe,p=0.8,list=FALSE)
training <- building[inTrain,]
validation <- building[-inTrain,]

# Datasets with all fields (except row number)
buildingFull <- pmlData[inBuild,]
testingFull <- pmlData[-inBuild,]
trainingFull <- buildingFull[inTrain,]
validationFull <- buildingFull[-inTrain,]

trainingdemo<-cbind(model.matrix(~0+classe+user_name,training),
                    select(training,-classe, -user_name))
```

The data were divided using the Caret package's createDataPartition function into a model building set (80%, n=``r nrow(building)``)) and a testing set (20% n=``r nrow(testing)``). The model building set is further divided into a training set (80% n=``r nrow(training)``) and a validation set (20% n=``r nrow(validation)``). (Terminology is that from Elements of Statistical Learning and Wikipedia, in the Coursera lectures the terms 'validation' and 'testing' are used the other way around.)

An initial exploratory analysis was carried out to determine what variables might
be important and whether a simple model could be used. There is no clear variable that might be thought to indicate the performance class prior to analysis. The user_name and classe
variables are factors, and have no natural ordering, so for exploration they were
converted to dummy variables. Correlation of the classe labels was calculated for a given user (Pedro) with all other variables.

```{r correl1}
# Correlation, exclude the user_names factor columns and remove the classe column
# self-correlation from results
corpedro <- cor(trainingdemo[trainingdemo$user_namepedro==1,-(6:10)])[1:5,6:57]
coreu <- cor(trainingdemo[trainingdemo$user_nameeurico==1,-(6:10)])[1:5,6:57]
maxel <- which.max(abs(corpedro))
maxelrow <- (maxel-1) %% 5 +1
maxelcol <- (maxel-1) %/% 5 +1
maxelval <- corpedro[maxelrow,maxelcol]
maxelcolname <- colnames(corpedro)[maxelcol]
maxelrowname <- rownames(corpedro)[maxelrow]

dispcolmin<-max(c(1,maxelcol-1))
dispcolmax<-min(c(maxelcol+1,ncol(corpedro)))
```

For this arbitrary user, the strongest correlation between any class and single variable was found. A sub-section of the correlation table is shown around this maximum (absolute) value of ``r round(maxelval,3)`` for ``r maxelrowname`` with ``r maxelcolname``.

```{r correl2}
kable(corpedro[,dispcolmin:dispcolmax],digits=3)
```

Plotting yaw\_belt against class for user Pedro, yaw\_belt seems to be a good indicator of class E. However across all users the separation is less clear.

```{r plotpedroyawclass}
plotYawPedro <- qplot(classe,yaw_belt,
                   data=filter(training,user_name=="pedro"),geom="boxplot",main="User Pedro")
plotYawAll <- qplot(classe,yaw_belt,
                    data=training,geom="boxplot", main="All users")
grid.arrange(plotYawPedro,plotYawAll,ncol=2)
```

Figure 1. Separation of classes by belt yaw.

This is because yaw_belt values are more different between users than across exercises for individual users.
```{r plotallyawuser}
qplot(user_name,yaw_belt,data=training,geom="boxplot", main="All classes")
```

Figure 2. Belt yaw values by user.

The distributions of yaw_belt for two different users are quite different, and not normally distributed. This suggests linear modelling will not perform well.

```{r plotpedroyawhist}
qplot(yaw_belt,data=filter(training,user_name %in% c("pedro","eurico")),
                           main="Yaw_belt distributions",bins=6) +
  facet_grid(scales="free",facets=~user_name)
```

Figure 3. Belt yaw distribution for two users.

A linear model is fitted to predict class B, using class A (exercise performed correctly) as the reference class. All variables, except class labels, are used. (A binomial GLM may be more appropriate, but does not perform very differently.)
```{r linearmodel}
lmB<-lm(classeB~.  -classeC - classeD - classeE,trainingdemo[,-1])
predictB <- 1*(predict(lmB,trainingdemo)>0.5)
cmB <- confusionMatrix(predictB,trainingdemo$classeB)
cmBTable <- table(data.frame(PredictB=
                               ifelse(predictB,"Predict B","Predict not B"),
                             ReferenceB=
                               ifelse(trainingdemo$classeB,"Ref B","Ref not B")))
cmBAcc <- cmB$overall[["Accuracy"]]

lmBcoefs <-summary(lmB)$coefficients
lmBpvals <- lmBcoefs[,"Pr(>|t|)"]
lmBpvalsAdj <- p.adjust(lmBpvals,method="bonferroni")
lmBpvalcount5pc <- sum(lmBpvalsAdj < 5e-2)
lmBpvalcount1M <- sum(lmBpvalsAdj < 1e-6)

lmBcoefsSig <- cbind(Estimate=lmBcoefs[,"Estimate"],
                     "Adjusted p"=lmBpvalsAdj)[lmBpvalsAdj<5e-2,]
```

The overall accuracy of this model is ``r round(cmBAcc,3)``, which seems relatively high, however the confusion matrix shows this is due to correctly predicting the large number of observations which are not class B. The accuracy for predicting class B observations is only about a third.
```{r linearmodelconftable, results='asis'}
kable(cmBTable)
```

After Bonferroni correction a total of ``r lmBpvalcount5pc`` variables show significant correlation with class B at a 5% level after Bonferroni correction, and ``r lmBpvalcount1M`` at a one-per-million level. As it appears difficult to find a meaningful explanation for classes from simple investigation of the data we attempt a principle component analysis.

The following are the first ten elements of the linear model that are significant at 5% corrected, the largest coefficients are for users.
```{r linearmodelcoeftable}
pander(signif(head(lmBcoefsSig,10),3))
```


```{r pca}
pcaAll <- select(training,-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcaP <- filter(training,user_name=="pedro") %>%
  select(-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcAll <- predict(pcaAll,training)
pcP <- predict(pcaP,filter(training,user_name=="pedro"))

```


Plotting the first two principle components it can be seen again that the users' values differ more from each other than between performance class. However there is difference within each user.

```{r plotpcaClasse, fig.width=10}
pStyle <- geom_point(alpha = 0.1)

plotPCAclass <- qplot(PC1,PC2,data=pcAll,color=classe,
                      main="Principal components, class groups") + pStyle
plotPCAuser <- qplot(PC1,PC2,data=pcAll,color=user_name,
                     main="Principal componants, user groups") + pStyle
grid.arrange(plotPCAclass, plotPCAuser, ncol=2)
```

Figure 4. Groups and classes divided by principal components.

# Machine learning models.

From exploratory analysis it is apparent that there is a high level of interaction between the different variables. We try seven methods of model fitting on the training dataset. These are:

* Random Forest (rf) and Stochastic Gradient Boosting (gbm) as a boosted random trees method.
* RF and GBM are also tested with principal components preprocessing.
* Again with RF and GBM separately, a model based on training a separate model to the data subsetted for each user (based on observation of the strong clustering of variables by user). To predict from this model, the previously fitted model for that user is selected and applied to the observation.
* Using a simple partitioning tree (rpart) on a previously discarded field, raw\_timestamp\_part\_1, which is not censored in the test data. Mainly out of annoyance that the entire assignment could be done in about five lines.

The same seed is set before each fitting procedure. All fitting is performed using a 10-fold cross-validation, which allows out of sample errors to be estimated. Due to the large sample size bootstrapping is not computationally efficient.

```{r trainmodels-set}
registerDoMC(cores = 3)
ctrl<-trainControl(method="cv",number=10)
```

```{r trainmodels-plain, eval=TRUE, cache=TRUE, results='hide'}
set.seed(501)
modFitRF<-train(classe~.,method="rf",data=training,trControl=ctrl)
set.seed(501)
modFitGBM<-train(classe~.,method="gbm",data=training,trControl=ctrl)
```
```{r trainmodels-pca, eval=TRUE, cache=TRUE, results='hide'}
set.seed(501)
modFitRFpca<-train(classe~.,method="rf",data=training,trControl=ctrl,preProcess="pca")
set.seed(501)
modFitGBMpca<-train(classe~.,method="gbm",data=training,trControl=ctrl,preProcess="pca")
```
```{r trainmodels-user, eval=TRUE, cache=TRUE, results='hide', warning=FALSE}
# Warnings suppressed as some values do not vary for some users
set.seed(501)
modFitGBMU <- list()
for (user in unique(training$user_name) ){
  datau<-filter(training,user_name==user)
  modFitGBMU[[user]]<-
    train(classe~.-user_name,method="gbm",data=filter(datau,user_name==user),trControl=ctrl)
}

set.seed(501)
modFitRFU <- list()
for (user in unique(training$user_name) ){
  datau<-filter(training,user_name==user)
  modFitRFU[[user]]<-
    train(classe~.-user_name,method="rf",data=filter(datau,user_name==user),trControl=ctrl)
}
```
```{r trainmodels-ts, eval=TRUE, cache=TRUE, results='hide'}
set.seed(501)
modFitTS <- train(classe~raw_timestamp_part_1, method="rpart", data=trainingFull, trControl=ctrl)
```
```{r trainmodels-accfns, eval=TRUE, results='hide', warnings=FALSE}
# Prediction function to use for the model lists.
predictU <- function(modlist,data) {
  pred <- character(nrow(data))
  for (user in unique(data$user_name)) {
    usersel <- data$user_name==user
    pred[usersel] <- as.character(predict(modlist[[user]],data[usersel,]))
    }
  pred
}

#mean(modFitRF$resample$Accuracy)
#getTrainPerf(modFitRF)

getAccCV <- function(x) {
  if ("train" %in% class(x)) {
    getTrainPerf(x)[1,1]
  } else {
    mean(sapply(x,function(Umod){getTrainPerf(Umod)[1,1]}))
  }
}

evalAccV <- function(x) {
  if ("train" %in% class(x)) {
    # Use validationFull so all models can find needed variables
    pred <- predict(x,validationFull)
  } else {
    pred <- predictU(x,validationFull)
  }
  confusionMatrix(validationFull$classe,pred)$overall["Accuracy"]
}
```
```{r trainmodels-acc, eval=TRUE, cache=TRUE}
accuracyCV <- sapply(list(RF=modFitRF, GBM=modFitGBM, "RF-PCA"=modFitRFpca,
                          "GBM-PCA"=modFitGBMpca, "RF-user"=modFitRFU,
                          "GBM-user"=modFitGBMU, "timestamp"=modFitTS),
                     getAccCV)


accuracyV <- sapply(list(RF=modFitRF, GBM=modFitGBM, "RF-PCA"=modFitRFpca,
                         "GBM-PCA"=modFitGBMpca, "RF-user"=modFitRFU,
                         "GBM-user"=modFitGBMU, "timestamp"=modFitTS),
                    evalAccV)
names(accuracyV)<-sub("\\.Accuracy","",names(accuracyV))
accuracyVboth <- rbind(Validation=accuracyV,"Cross validation"=accuracyCV)
```

# Results

The out of sample errors are compared using the validation dataset. The 10-fold cross-validation accuracy for each model is also shown.
```{r evalmodels}
kable(accuracyVboth, digits=4)
```

The best performing model in the validation set is the GBM-user model. Cross validation and outside validation results are similar, especially for the higher performing models. (The single variable raw timestamp model is very close, but using it seems against the spirit of the assignment.) Before applying this model to the test set we re-train on the full building set (which has already been used to determine the model in any case).
```{r trainmodels-final, eval=TRUE, cache=TRUE, results='hide', warning=FALSE}
# Warnings suppressed as some values do not vary for some users
set.seed(501)
modFitGBMUFinal <- list()
for (user in unique(training$user_name) ){
  datau<-filter(building,user_name==user)
  modFitGBMUFinal[[user]]<-
    train(classe~.-user_name,method="gbm",data=filter(datau,user_name==user),trControl=ctrl)
}
testPredict <- predictU(modFitGBMUFinal,testing)
testAcc <- confusionMatrix(testing$classe,testPredict)$overall["Accuracy"]
cvAccfinal <- getAccCV(modFitGBMUFinal)
impdf <- data.frame(nrow=nrow(varImp(modFitGBMUFinal[[1]])$importance))
for (U in names(modFitGBMUFinal)) { impdf<-cbind(varImp(modFitGBMUFinal[[U]])$importance)}
impmean<-apply(as.matrix(impdf),1,mean)
impmean<-impmean[order(impmean)]
```

The final expected out-of-sample error is ``r round(testAcc,5)`` from estimation on the test sample, ``r round(cvAccfinal,5)`` from cross-validation during fitting. This is the expected accuracy when applying to an unseen data set. (There is the potential for upward bias in the cross-validation result, as this method was selected on the basis of its higher cross validation accuracy on largely the same data.) The importance of variables in the final fit is shown below,

```{r plot-importance-final, fig.height=5}
op <- par(mar = c(3, 10, 2, 2) + 0.1)
barplot(tail(impmean,30),horiz=TRUE, las=1)
par(op)
```

Figure 5. Top 30 variables in final fit. The user label is used in model selection so is not included in importance measurement.

```{r predictblinded, warning=FALSE, results='hide'}
suppressMessages(finalPredict <- predictU(modFitGBMUFinal,pmlDataTest))
suppressMessages(finalPredict <- data.frame(X=pmlDataTest$X,classe=finalPredict))
write.csv(finalPredict,"predictions.csv")
```