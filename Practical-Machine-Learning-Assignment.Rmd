---
title: "Predicting quality of exercise action"
author: "Ian"
date: "17 January 2016"
output: html_document
---

# Summary
A weight lifting excercise dataset, [Velloso et al.][Velloso] with on-body sensing data for 6 participants, performing lifts categorised by how well/the type of mistakes made (performance classe) was investigated with the goal of predicting the performance class of 20 randomly selected observations. The unblinded data were split into training, validation and testing subsets. After exploratory analysis on the training set it was used to build a range of predictive models in an automated fashion using the Caret package. The performance of these was compared on the validation set and the best performing one selected for use in predicting the class of the blinded test observations. Its expected accuracy for this was mesured using the remaining unblinded test data.

[Velloso]: http://groupware.les.inf.puc-rio.br/har#ixzz3xpfM3Y37 "(Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.)"

# Data description

In the original dataset, the participants carried out repetitions of a single dumbbell arm curl task under supervision. They were either carried correctly (class A) or in a range of intentionally incorrect manners (classes B-E) under supervision. Sensors were mounted on the glove, armband, belt and dumbbell, each reporting acceleration, magnetometer and gyroscope information on three axes. The original researchers also divided the observations (at 45Hz) into time windows and calculated summary statistics for each time window. The data were retrieved from the Coursera Practical Machine Learning exercise and loaded into R.

```{r libraries}
# Can't include in cached chunks.
library(dplyr, quietly=TRUE, warn.conflicts = FALSE)
library(caret, quietly=TRUE)
library(knitr, quietly=TRUE)
library(doMC)
```


```{r loaddata, cache=TRUE}
pmlData<-read.csv("pml-training.csv",as.is=TRUE)
pmlDataTest<-read.csv("pml-testing.csv",as.is=TRUE)
pmlDataFilt<-filter(pmlData,new_window=="no")
delcol <- apply(pmlDataFilt,2,function(x){length(unique(x))==1})
delcolind <- which(delcol)
delcolnames <- names(pmlData)[delcol]

pmlData <- pmlData %>%
  select(-delcolind) %>%
  mutate(classe=as.factor(classe),user_name=as.factor(user_name))

pmlDataTest <- pmlDataTest %>%
  mutate(user_name=as.factor(user_name))

pmlDataSel <- pmlData %>%
  select(-c(X, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp,
            num_window))
```

The original data has 160 variables from different sensors, and time-window summary data for the last observation in each time window. Since the prediction task we are addressing does not include full time series data, but only single observations, we discard these summary variables by removing all fields that have only one unique value when the "new window" observations are omitted. Additionally the fields relating to observation time are removed (row number X, raw\_timestamp\_part\_1, raw\_timestamp\_part\_2, cvtd\_timestamp and num\_window). These variables have been removed from the prediction set. This leaves 54 variables including the "classe" variable, indicating how the task was carried out, which we wish to be able to predict. The 106 fields removed are: the 96 new features described in Velloso et al. (for four devices, eight parameters for each Euler angle (3)), the five fields mentioned already, new\_window (whether the observation includes window summary) and four additional derived features corresponding to average acceleration for each measurement device, not mentioned in Velloso et al.

# Data partitioning and exploratory analysis

```{r trainingset}
set.seed(500)
inBuild <- createDataPartition(pmlDataSel$classe,p=0.8,list=FALSE)
building <- pmlDataSel[inBuild,]
testing <- pmlDataSel[-inBuild,]
inTrain <- createDataPartition(building$classe,p=0.8,list=FALSE)
training <- building[inTrain,]
validation <- building[-inTrain,]

# Datasets with all fields (except row number)
buildingFull <- pmlData[inBuild,]
testingFull <- pmlData[-inBuild,]
trainingFull <- buildingFull[inTrain,]
validationFull <- buildingFull[-inTrain,]

trainingdemo<-cbind(model.matrix(~0+classe+user_name,training),
                    select(training,-classe, -user_name))
```

The data are divided using the Caret package's createDataPartition function into a model building set (80%, n=``r nrow(building)``)) and a testing set (20% n=``r nrow(testing)``). The model building set is further divided into a training set (80% n=``r nrow(training)``) and a validation set (20% n=``r nrow(validation)``). (Terminology is that from Elements of Statistical Learning and Wikipedia, in the Coursera lectures the terms 'validation' and 'testing' are used the other way around.)

An initial exporatory analysis was carried out to determine what variables might
be important and whether a simple model could be used. There is no clear variable that might be thought to indicate the performance class prior to analysis. The user_name and classe
variables are factors, and have no natural ordering, so for exploration they are
converted to dummy variables and covariance of the classe labels with all other variables
calculated for a given user.

```{r correl1}
# Correlation, exclude the user_names factor columns and remove the classe column
# self-correlation from results
corpedro <- cor(trainingdemo[trainingdemo$user_namepedro==1,-(6:10)])[1:5,6:57]
coreu <- cor(trainingdemo[trainingdemo$user_nameeurico==1,-(6:10)])[1:5,6:57]
maxel <- which.max(abs(corpedro))
maxelrow <- (maxel-1) %% 5 +1
maxelcol <- (maxel-1) %/% 5 +1
maxelval <- corpedro[maxelrow,maxelcol]
maxelcolname <- colnames(corpedro)[maxelcol]
maxelrowname <- rownames(corpedro)[maxelrow]

dispcolmin<-max(c(1,maxelcol-1))
dispcolmax<-min(c(maxelcol+1,ncol(corpedro)))
```

Choosing an arbitrary user (Pedro), the strongest correlation between any class and single variable was found. A sub-section of the correlation table is shown around this maximum (absolute) value of ``r round(maxelval,3)`` for ``r maxelrowname`` with ``r maxelcolname``.

```{r correl2}
round(corpedro[,dispcolmin:dispcolmax],3)
```

For user Pedro, yaw\_belt seems to be a good indicator of class E, plotting yaw\_belt against class:
```{r plotpedroyawclass}
qplot(classe,yaw_belt,data=filter(training,user_name=="pedro"),geom="boxplot",main="User Pedro")
```

However across all users the separation is less clear, plotting yaw_\belt against class for all users:
```{r plotallyawclass}
qplot(classe,yaw_belt,data=training,geom="boxplot", main="All users")
```

This is because yaw_belt values are more different between users than across exercises for individual users.
```{r plotallyawuser}
qplot(user_name,yaw_belt,data=training,geom="boxplot", main="All classes")
```

For a different user, Eurico, the yaw_belt variable does not distinguish class E, and is more associated with class D.
```{r ploteuricoyawclass}
qplot(classe,yaw_belt,data=filter(training,user_name=="eurico"),geom="boxplot",main="User Eurico")
```

The distributions of yaw_belt for these two users are quite different, and not normally distributed. This suggests linear modelling will not perform well.
```{r plotpedroyawhist}
qplot(yaw_belt,data=filter(training,user_name %in% c("pedro","eurico")),
                           main="Yaw_belt distributions",bins=6) +
  facet_grid(scales="free",facets=~user_name)
```

A linear model is fitted to predict class B, using class A (exercise performed correctly) as the reference class. All variables, except class labels, are used. (A binomial GLM may be more appropriate, but does not perform very differently.)
```{r linearmodel}
lmB<-lm(classeB~.  -classeC - classeD - classeE,trainingdemo[,-1])
cmB <- confusionMatrix(1*(predict(lmB,trainingdemo)>0.5),trainingdemo$classeB)
cmBAcc <- cmB$overall[["Accuracy"]]

lmBcoefs <-summary(lmB)$coefficients
lmBpvals <- lmBcoefs[,"Pr(>|t|)"]
lmBpvalsAdj <- p.adjust(lmBpvals,method="bonferroni")
lmBpvalcount5pc <- sum(lmBpvalsAdj < 5e-2)
lmBpvalcount1M <- sum(lmBpvalsAdj < 1e-6)

lmBcoefsSig <- cbind(Estimate=lmBcoefs[,"Estimate"],
                     "Adjusted p"=lmBpvalsAdj)[lmBpvalsAdj<5e-2,]
```

The overall accuracy of this model is ``r round(cmBAcc,3)``, which seems relatively high, however the confusion matrix shows this is due to correctly predicting the large number of observations which are not class B. The accuracy for predicting class B observations is only about a third.
```{r linearmodelconftable}
cmB$table
```

After Bonferroni correction a total of ``r lmBpvalcount5pc`` variables show significant correlation with class B at a 5% level after Bonferroni correction, and ``r lmBpvalcount1M`` at a one-per-million level. As it appears difficult to find a meaningful explanation for classes from simple investigation of the data we attempt a principle component analysis.

The first ten elements of the linear model that are significant at 5% corrected, the largest coefficents are for users.
```{r linearmodelcoeftable}
library(pander)
pander(head(signif(lmBcoefsSig,3),10))
```


```{r pca}
pcaAll <- select(training,-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcaP <- filter(training,user_name=="pedro") %>%
  select(-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcAll <- predict(pcaAll,training)
pcP <- predict(pcaP,filter(training,user_name=="pedro"))

```


Plotting the first two principle components it can be seen again that the users' values differ more from each other than between performance class. However there is difference within each user.
```{r plotpcaClasse}
qplot(PC1,PC2,data=pcAll,color=classe, main="Principal components, class groups")
```
```{r plotpcaUser}
qplot(PC1,PC2,data=pcAll,color=user_name, main="Principal componants, user groups")
```

# Machine learning models.

From exploratory analysis it is apparent that there is a high level of interaction between the different variables. We try seven methods of model fitting on the training dataset. These are:
* Random Forest (rf) and Stochastic Gradient Boosting (gbm) as a boosted random trees method.
* RF and GBM are also tested with principal components preprocessing.
* Using RF and GBM, two models based on fitting a separate model to the data subsetted for each user (based on observation of the strong clustering of variables by user). To predict from this model, the previously fitted model for that user is selected and applied to the observation.
* Using a simple partitioning tree (rpart) on a previously discarded field, raw\_timestamp\_part\_1, which is not censored in the test data. Mainly to demonstrate annoyance that the entire assignment could be done in about five lines.

The same seed is set before each fitting procedure. All fitting is performed using a 10-fold cross-validation, which allows out of sample errors to be estimated. Due to the large sample size bootstrapping is not computationally efficient.

```{r trainmodels-set}
registerDoMC(cores = 3)
ctrl<-trainControl(method="cv",number=10)
```

```{r trainmodels-plain, eval=TRUE, cache=TRUE}
set.seed(501)
modFitRF<-train(classe~.,method="rf",data=training,trControl=ctrl)
set.seed(501)
modFitGBM<-train(classe~.,method="gbm",data=training,trControl=ctrl)
```
```{r trainmodels-pca, eval=TRUE, cache=TRUE}
set.seed(501)
modFitRFpca<-train(classe~.,method="rf",data=training,trControl=ctrl,preProcess="pca")
set.seed(501)
modFitGBMpca<-train(classe~.,method="gbm",data=training,trControl=ctrl,preProcess="pca")
```
```{r trainmodels-user, eval=TRUE, cache=TRUE}
set.seed(501)
modFitGBMU <- list()
for (user in unique(training$user_name) ){
  datau<-filter(training,user_name==user)
  modFitGBMU[[user]]<-
    train(classe~.-user_name,method="gbm",data=filter(datau,user_name==user),trControl=ctrl)
}

set.seed(501)
modFitRFU <- list()
for (user in unique(training$user_name) ){
  datau<-filter(training,user_name==user)
  modFitRFU[[user]]<-
    train(classe~.-user_name,method="rf",data=filter(datau,user_name==user),trControl=ctrl)
}
```
```{r trainmodels-ts, eval=TRUE, cache=TRUE}
set.seed(501)
modFitTS <- train(classe~raw_timestamp_part_1, method="rpart", data=trainingFull, trControl=ctrl)
```
```{r trainmodels-acc, eval=TRUE, cache=TRUE}
# Prediction function to use for the model lists.
predictU <- function(modlist,data) {
  pred <- character(nrow(data))
  for (user in unique(data$user_name)) {
    usersel <- data$user_name==user
    pred[usersel] <- as.character(predict(modlist[[user]],data[usersel,]))
    }
  pred
}

#mean(modFitRF$resample$Accuracy)
#getTrainPerf(modFitRF)

getAccCV <- function(x) {
  if ("train" %in% class(x)) {
    getTrainPerf(x)[1,1]
  } else {
    mean(sapply(x,function(Umod){getTrainPerf(Umod)[1,1]}))
  }
}

evalAccV <- function(x) {
  if ("train" %in% class(x)) {
    # Use validationFull so all models can find needed variables
    pred <- predict(x,validationFull)
  } else {
    pred <- predictU(x,validationFull)
  }
  confusionMatrix(validationFull$classe,pred)$overall["Accuracy"]
}

accuracyCV <- sapply(list(RF=modFitRF, GBM=modFitGBM, "RF-PCA"=modFitRFpca,
                          "GBM-PCA"=modFitGBMpca, "RF-user"=modFitRFU,
                          "GBM-user"=modFitGBMU, "timestamp"=modFitTS),
                     getAccCV)


accuracyV <- sapply(list(RF=modFitRF, GBM=modFitGBM, "RF-PCA"=modFitRFpca,
                         "GBM-PCA"=modFitGBMpca, "RF-user"=modFitRFU,
                         "GBM-user"=modFitGBMU, "timestamp"=modFitTS),
                    evalAccV)
names(accuracyV)<-sub("\\.Accuracy","",names(accuracyV))
accuracyVboth <- rbind(Validation=accuracyV,"Cross validation"=accuracyCV)
```

The out of sample errors are compared on the validation dataset.
```{r evalmodels}
kable(accuracyVboth, digits=4)
```

The best performing model in the validation set and cross validation is the GBM-user model. Cross validation and outside validation results are similar, especially for the higher performing models. (The single variable raw timestamp model is very close, but using it seems against the spirit of the assignment.) Before applying it to the test set we re-train on the full building set.
```{r trainmodels-final, eval=TRUE, cache=TRUE}
set.seed(501)
modFitGBMUFinal <- list()
for (user in unique(training$user_name) ){
  datau<-filter(building,user_name==user)
  modFitGBMUFinal[[user]]<-
    train(classe~.-user_name,method="gbm",data=filter(datau,user_name==user),trControl=ctrl)
}
testPredict <- predictU(modFitGBMUFinal,testing)
testAcc <- confusionMatrix(testing$classe,testPredict)$overall["Accuracy"]
cvAccfinal <- getAccCV(modFitGBMUFinal)
```

The final expected out-of-sample error is ``r round(testAcc,4)`` from estimation on the test sample, ``r round(cvAccfinal)`` from cross-validation during fitting. (There is the potential for upward bias in the cross-validation result, as this method was selected on the basis of its higher cross validation accuracy on largely the same data.)

```{r predict-blinded}
finalPredict <- predictU(modFitGBMUFinal,pmlDataTest)
finalPredict <- data.frame(X=pmlDataTest$X,classe=finalPredict)
write.csv(finalPredict,"predictions.csv")
```