---
title: "Practical-Machine-Learning-Assignment"
author: "Ian"
date: "17 January 2016"
output: html_document
---

A weight lifting excercise dataset, (Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.) with on-body sensing data for 6 participants, performing lifts categorised by how well/the type of mistakes made (performance classe) was investigated and a predictive model built. We analysed this data with the goal of being able to predict the perfomance class in 20 randomly selected test observations.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3xpfM3Y37


```{r libraries}
# Can't include in cached chunks.
library(dplyr, quietly=TRUE)
library(caret, quietly=TRUE)
```


```{r loaddata, cache=TRUE}
pmlData<-read.csv("pml-training.csv",as.is=TRUE)
pmlDataFilt<-filter(pmlData,new_window=="no")
delcol <- apply(pmlDataFilt,2,function(x){length(unique(x))==1})
delcolind <- which(delcol)
delcolnames <- names(pmlData)[delcol]

pmlDataSel <- pmlData %>%
  select(-delcolind) %>%
  select(-c(X, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp,
            num_window)) %>%
  mutate(classe=as.factor(classe),user_name=as.factor(user_name))
```

The original data has 160 variables from different sensors, and time-window summary data for some records inserted by the original researchers. Since the prediction task we are addressing does not include full time series data, but only single observations, we discard these summary variables by removing all fields that have only one unique value when the "new window" observations are omitted. Additionally the fields relating to observation time are removed (observation number X, raw\_timestamp\_part\_1, raw\_timestamp\_part\_2, cvtd\_timestamp and num\_window). These variables have been removed from the prediction set. This leaves 54 variables including the "classe" variable, indicating how the task was carried out, which we wish to be able to predict. The 106 fields removed are: the 96 new features described in Velloso et al. (for four devices, eight parameters for each Euler angle (3)), the five fields mentioned already, new\_window (only one value after removal of the window summary rows) and four additional derived features corresponding to average acceleration for each measurement device, not mentioned in Velloso et al.

```{r trainingset}
set.seed(500)
inBuild <- createDataPartition(pmlDataSel$classe,p=0.8,list=FALSE)
building <- pmlDataSel[inBuild,]
testing <- pmlDataSel[-inBuild,]
inTrain <- createDataPartition(building$classe,p=0.8,list=FALSE)
training <- building[inTrain,]
validation <- building[-inTrain,]

trainingdemo<-cbind(model.matrix(~0+classe+user_name,training),
                    select(training,-classe, -user_name))
```

The data are divided using the Caret package's createDataPartition function into a model building set (80%, n=``r nrow(building)``)) and a testing set (20% n=``r nrow(testing)``). The model building set is further divided into a training set (80% n=``r nrow(training)``) and a validation set (20% n=``r nrow(validation)``). Note terminology is that from Elements of Statistical Learning and Wikipedia, Brian Caffo's lectures use the terms 'validation' and 'testing' the other way around.)

An initial exporatory analysis was carried out to determine what variables might
be important and whether a simple model could be used. There is no clear variable that might be thought to indicate the performance class prior to analysis. The user_name and classe
variables are factors, and have no natural ordering, so for exploration they are
converted to dummy variables and covariance of the classe labels with all other variables
calculated for a given user.


```{r correl1}
# Correlation, exclude the user_names factor columns and remove the classe column
# self-correlation from results
corpedro <- cor(trainingdemo[trainingdemo$user_namepedro==1,-(6:10)])[1:5,6:57]
coreu <- cor(trainingdemo[trainingdemo$user_nameeurico==1,-(6:10)])[1:5,6:57]
maxel <- which.max(abs(corpedro))
maxelrow <- (maxel-1) %% 5 +1
maxelcol <- (maxel-1) %/% 5 +1
maxelval <- corpedro[maxelrow,maxelcol]
maxelcolname <- colnames(corpedro)[maxelcol]
maxelrowname <- rownames(corpedro)[maxelrow]

dispcolmin<-max(c(1,maxelcol-1))
dispcolmax<-min(c(maxelcol+1,ncol(corpedro)))
```

Picking an arbitrary user (Pedro), the highest correlation between any class and single variable was found. A sub-section of the correlation table is shown around this maximum (absolute) value of ``r round(maxelval,3)`` for ``r maxelrowname`` with ``r maxelcolname``.

```{r correl2}
round(corpedro[,dispcolmin:dispcolmax],3)
```

For user Pedro, yaw_belt seems to be a good indicator of class E.
```{r plotpedroyawclass}
qplot(classe,yaw_belt,data=filter(training,user_name=="pedro"),geom="boxplot")
```

However across all users the separation is less clear.
```{r plotallyawclass}
qplot(classe,yaw_belt,data=training,geom="boxplot")
```

This is because yaw_belt values are more different between users than across exercises for individual users.
```{r plotallyawuser}
qplot(user_name,yaw_belt,data=training,geom="boxplot")
```

For a different user, Eurico, the yaw_belt variable does not distinguish class E, and is more associated with class D.
```{r ploteuricoyawclass}
qplot(classe,yaw_belt,data=filter(training,user_name=="eurico"),geom="boxplot")
```

The distributions of yaw_belt for these two users are quite different, and not normally distributed. This suggests linear modelling will not perform well.
```{r plotpedroyawhist}
qplot(yaw_belt,data=filter(training,user_name %in% c("pedro","eurico")),bins=6) +
  facet_grid(scales="free",facets=~user_name)
```

A linear model is fitted to predict class B, using class A (exercise performed correctly) as the reference class. All variables, except class labels, are used. (A binomial GLM may be more appropriate, but does not perform very differently.)
```{r linearmodel}
lmB<-lm(classeB~.  -classeC - classeD - classeE,trainingdemo[,-1])
cmB <- confusionMatrix(1*(predict(lmB,trainingdemo)>0.5),trainingdemo$classeB)
cmBAcc <- cmB$overall[["Accuracy"]]

lmBpvals <- summary(lmB)$coefficients[,"Pr(>|t|)"]
lmBpvalsAdj <- p.adjust(lmBpvals,method="bonferroni")
lmBpvalcount5pc <- sum(lmBpvalsAdj < 5e-2)
lmBpvalcount1M <- sum(lmBpvalsAdj < 1e-6)

```

The overall accuracy of this model is ``r round(cmBAcc,3)``, which seems relatively high, however the confusion matrix shows this is due to correctly predicting the large number of observations which are not class B. The accuracy for predicting class B observations is only about a third.
```{r linearmodeltable}
cmB$table
```

After Bonferroni correction a total of ``r lmBpvalcount5pc`` variables show significant correlation with class B at a 5% level after Bonferroni correction, and ``r lmBpvalcount1M`` at a one-per-million level. As it appears difficult to find a meaningful explanation for classes from simple investigation of the data we attempt a principle component analysis.

```{r pca}
pcaAll <- select(training,-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcaP <- filter(training,user_name=="pedro") %>%
  select(-classe,-user_name) %>%
  preProcess(method="pca",pcaComp = 2)
pcAll <- predict(pcaAll,training)
pcP <- predict(pcaP,filter(training,user_name=="pedro"))

```


Plotting the first two principle components it can be seen again that the users' values differ more from each other than between performance class. However there is difference within each user.
```{r plotpcaClasse}
qplot(PC1,PC2,data=pcAll,color=classe)
```
```{r plotpcaUser}
qplot(PC1,PC2,data=pcAll,color=user_name)
```


```{r trainmodels, eval=FALSE}
set.seed(501)
#ctrl<-trainControl(method="repeatedcv",repeats=5)
ctrl<-trainControl(method="cv",number=10)
ctrlBoot<-trainControl(number=10,method="boot")
modknn<-train(classe~.,method="knn",data=training,trControl=ctrlBoot)
timeGBMb<-system.time(modFitGBM<-train(classe~.,method="gbm",data=training,trControl=ctrlBoot,verbose=FALSE))


timeRF<-system.time(modFitRF<-train(classe~.,method="rf",data=training,trControl=ctrl))
timeGBM<-system.time(modFitGBM<-train(classe~.,method="gbm",data=training,trControl=ctrl))
timeRFpca<-system.time(modFitRFpca<-train(classe~.,method="rf",data=training,trControl=ctrl,preProcess="pca"))
timeGBMpca<-system.time(modFitGBMpca<-train(classe~.,method="gbm",data=training,trControl=ctrl,preProcess="pca"))
```
